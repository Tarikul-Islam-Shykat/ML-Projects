{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Package","metadata":{}},{"cell_type":"code","source":"!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:03.971532Z","iopub.execute_input":"2023-07-14T06:47:03.972534Z","iopub.status.idle":"2023-07-14T06:47:16.162295Z","shell.execute_reply.started":"2023-07-14T06:47:03.972498Z","shell.execute_reply":"2023-07-14T06:47:16.161174Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:16.174206Z","iopub.execute_input":"2023-07-14T06:47:16.174755Z","iopub.status.idle":"2023-07-14T06:47:17.207253Z","shell.execute_reply.started":"2023-07-14T06:47:16.174700Z","shell.execute_reply":"2023-07-14T06:47:17.206031Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Fri Jul 14 06:47:17 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   58C    P0    46W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, set_seed\n\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom tqdm import tqdm\nimport torch\n\nnltk.download(\"punkt\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:17.221706Z","iopub.execute_input":"2023-07-14T06:47:17.222335Z","iopub.status.idle":"2023-07-14T06:47:29.484196Z","shell.execute_reply.started":"2023-07-14T06:47:17.222289Z","shell.execute_reply":"2023-07-14T06:47:29.483201Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading Model ","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice ","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:29.485622Z","iopub.execute_input":"2023-07-14T06:47:29.487515Z","iopub.status.idle":"2023-07-14T06:47:29.549864Z","shell.execute_reply.started":"2023-07-14T06:47:29.487475Z","shell.execute_reply":"2023-07-14T06:47:29.548771Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"model_ckpt = \"google/pegasus-cnn_dailymail\"","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:29.552244Z","iopub.execute_input":"2023-07-14T06:47:29.553444Z","iopub.status.idle":"2023-07-14T06:47:29.559750Z","shell.execute_reply.started":"2023-07-14T06:47:29.553407Z","shell.execute_reply":"2023-07-14T06:47:29.558745Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) # to(device) measn its just assigning gpu","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:29.561819Z","iopub.execute_input":"2023-07-14T06:47:29.562981Z","iopub.status.idle":"2023-07-14T06:47:51.635032Z","shell.execute_reply.started":"2023-07-14T06:47:29.562947Z","shell.execute_reply":"2023-07-14T06:47:51.633936Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"dataset_samsum = load_dataset(\"samsum\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:51.641306Z","iopub.execute_input":"2023-07-14T06:47:51.641602Z","iopub.status.idle":"2023-07-14T06:47:52.699609Z","shell.execute_reply.started":"2023-07-14T06:47:51.641577Z","shell.execute_reply":"2023-07-14T06:47:52.698572Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462e83838e274b58b0521efbb26481de"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:52.700980Z","iopub.execute_input":"2023-07-14T06:47:52.701987Z","iopub.status.idle":"2023-07-14T06:47:52.708641Z","shell.execute_reply.started":"2023-07-14T06:47:52.701949Z","shell.execute_reply":"2023-07-14T06:47:52.707655Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Traing test valiadion ","metadata":{}},{"cell_type":"code","source":"split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\nsplit_lengths","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:52.709915Z","iopub.execute_input":"2023-07-14T06:47:52.710652Z","iopub.status.idle":"2023-07-14T06:47:52.721916Z","shell.execute_reply.started":"2023-07-14T06:47:52.710619Z","shell.execute_reply":"2023-07-14T06:47:52.720918Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[14732, 819, 818]"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\nprint(\"\\nDialogue:\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:52.723189Z","iopub.execute_input":"2023-07-14T06:47:52.725548Z","iopub.status.idle":"2023-07-14T06:47:52.733602Z","shell.execute_reply.started":"2023-07-14T06:47:52.725513Z","shell.execute_reply":"2023-07-14T06:47:52.732471Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Split lengths: [14732, 819, 818]\nFeatures: ['id', 'dialogue', 'summary']\n\nDialogue:\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset_samsum[\"test\"][1][\"dialogue\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:52.735797Z","iopub.execute_input":"2023-07-14T06:47:52.737067Z","iopub.status.idle":"2023-07-14T06:47:52.744370Z","shell.execute_reply.started":"2023-07-14T06:47:52.737033Z","shell.execute_reply":"2023-07-14T06:47:52.743118Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Eric: MACHINE!\nRob: That's so gr8!\nEric: I know! And shows how Americans see Russian ;)\nRob: And it's really funny!\nEric: I know! I especially like the train part!\nRob: Hahaha! No one talks to the machine like that!\nEric: Is this his only stand-up?\nRob: Idk. I'll check.\nEric: Sure.\nRob: Turns out no! There are some of his stand-ups on youtube.\nEric: Gr8! I'll watch them now!\nRob: Me too!\nEric: MACHINE!\nRob: MACHINE!\nEric: TTYL?\nRob: Sure :)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"\\nSummary:\")\nprint(dataset_samsum[\"test\"][1][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:52.746942Z","iopub.execute_input":"2023-07-14T06:47:52.748295Z","iopub.status.idle":"2023-07-14T06:47:52.755539Z","shell.execute_reply.started":"2023-07-14T06:47:52.748260Z","shell.execute_reply":"2023-07-14T06:47:52.754449Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\nSummary:\nEric and Rob are going to watch a stand-up on youtube.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Evaluating model PEGASUS on SAMSum data","metadata":{}},{"cell_type":"code","source":"dataset_samsum['test'][2]['dialogue']","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:52.757659Z","iopub.execute_input":"2023-07-14T06:47:52.759057Z","iopub.status.idle":"2023-07-14T06:47:52.767171Z","shell.execute_reply.started":"2023-07-14T06:47:52.759019Z","shell.execute_reply":"2023-07-14T06:47:52.766079Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"Lenny: Babe, can you help me with something?\\r\\nBob: Sure, what's up?\\r\\nLenny: Which one should I pick?\\r\\nBob: Send me photos\\r\\nLenny:  <file_photo>\\r\\nLenny:  <file_photo>\\r\\nLenny:  <file_photo>\\r\\nBob: I like the first ones best\\r\\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\\r\\nBob: I have four black pairs :D :D\\r\\nLenny: yeah, but shouldn't I pick a different color?\\r\\nBob: what matters is what you'll give you the most outfit options\\r\\nLenny: So I guess I'll buy the first or the third pair then\\r\\nBob: Pick the best quality then\\r\\nLenny: ur right, thx\\r\\nBob: no prob :)\""},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline('summarization', model = model_ckpt )\npipe_out = pipe(dataset_samsum['test'][0]['dialogue'] )\nprint(pipe_out)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:47:52.769269Z","iopub.execute_input":"2023-07-14T06:47:52.770573Z","iopub.status.idle":"2023-07-14T06:48:32.886180Z","shell.execute_reply.started":"2023-07-14T06:47:52.770469Z","shell.execute_reply":"2023-07-14T06:48:32.885113Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': \"Amanda: Ask Larry Amanda: He called her last time we were at the park together .<n>Hannah: I'd rather you texted him .<n>Amanda: Just text him .\"}]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(pipe_out[0]['summary_text'].replace(\" .<n>\", \".\\n\"))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:48:32.887869Z","iopub.execute_input":"2023-07-14T06:48:32.888563Z","iopub.status.idle":"2023-07-14T06:48:32.894473Z","shell.execute_reply.started":"2023-07-14T06:48:32.888523Z","shell.execute_reply":"2023-07-14T06:48:32.893504Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Amanda: Ask Larry Amanda: He called her last time we were at the park together.\nHannah: I'd rather you texted him.\nAmanda: Just text him .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Calculate the metric\nthis methow will give score the of the ","metadata":{}},{"cell_type":"code","source":"def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, \n                               batch_size=16, device=device, \n                               column_text=\"article\", \n                               column_summary=\"highlights\"):\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n        \n        inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n                        padding=\"max_length\", return_tensors=\"pt\")\n        \n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                         attention_mask=inputs[\"attention_mask\"].to(device), \n                         length_penalty=0.8, num_beams=8, max_length=128)\n        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n        \n        # Finally, we decode the generated texts, \n        # replace the  token, and add the decoded texts with the references to the metric.\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                                clean_up_tokenization_spaces=True) \n               for s in summaries]      \n        \n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n        \n        \n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n        \n    #  Finally compute and return the ROUGE scores.\n    score = metric.compute()\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:48:32.896100Z","iopub.execute_input":"2023-07-14T06:48:32.896788Z","iopub.status.idle":"2023-07-14T06:48:32.909401Z","shell.execute_reply.started":"2023-07-14T06:48:32.896744Z","shell.execute_reply":"2023-07-14T06:48:32.908323Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Split the dataset\nsplit the dataset into smaller batches that we can process simultaneously Yield successive batch-sized chunks from list_of_elements. Passing the data in batch wise ","metadata":{}},{"cell_type":"code","source":"def generate_batch_sized_chunks(list_of_elements, batch_size):\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:48:32.910812Z","iopub.execute_input":"2023-07-14T06:48:32.911487Z","iopub.status.idle":"2023-07-14T06:48:32.931490Z","shell.execute_reply.started":"2023-07-14T06:48:32.911445Z","shell.execute_reply":"2023-07-14T06:48:32.930453Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Calculate on the samsum data ","metadata":{}},{"cell_type":"code","source":"rouge_metric = load_metric('rouge')\nscore = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text = 'dialogue', column_summary='summary', batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T06:48:32.933061Z","iopub.execute_input":"2023-07-14T06:48:32.933766Z","iopub.status.idle":"2023-07-14T07:00:56.486934Z","shell.execute_reply.started":"2023-07-14T06:48:32.933711Z","shell.execute_reply":"2023-07-14T07:00:56.485794Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 103/103 [12:18<00:00,  7.17s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\ndataset_samsum['test'] : this is the test dataset.\n\nrouge_metric = load_metric('rouge')\nROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics commonly used for evaluating the quality of text summarization or machine translation outputs. \nIt measures the overlap between the generated summary and a reference summary based on n-gram matches and other criteria.\n\nmodel_pegasus : model's that are trained on.\n\ntokenizer : tokenizer from hugging face.\n'''","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:00:56.488869Z","iopub.execute_input":"2023-07-14T07:00:56.489245Z","iopub.status.idle":"2023-07-14T07:00:56.496593Z","shell.execute_reply.started":"2023-07-14T07:00:56.489211Z","shell.execute_reply":"2023-07-14T07:00:56.495466Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\"\\ndataset_samsum['test'] : this is the test dataset \\n\\nrouge_metric = load_metric('rouge')\\nROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics commonly used for evaluating the quality of text summarization or machine translation outputs. \\nIt measures the overlap between the generated summary and a reference summary based on n-gram matches and other criteria.\\n\\nmodel_pegasus : model's that are trained on \\n\\ntokenizer : tokenizer from hugging face\\n\""},"metadata":{}}]},{"cell_type":"code","source":"rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n\npd.DataFrame(rouge_dict, index = ['pegasus'])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:00:56.498180Z","iopub.execute_input":"2023-07-14T07:00:56.498902Z","iopub.status.idle":"2023-07-14T07:00:56.522667Z","shell.execute_reply.started":"2023-07-14T07:00:56.498864Z","shell.execute_reply":"2023-07-14T07:00:56.521771Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"           rouge1    rouge2    rougeL  rougeLsum\npegasus  0.015531  0.000298  0.015506   0.015484","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.015531</td>\n      <td>0.000298</td>\n      <td>0.015506</td>\n      <td>0.015484</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dialogue_token_len = len([tokenizer.encode(s) for s in dataset_samsum['train']['dialogue']])\ndialogue_token_len","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:00:56.524123Z","iopub.execute_input":"2023-07-14T07:00:56.524824Z","iopub.status.idle":"2023-07-14T07:01:03.512934Z","shell.execute_reply.started":"2023-07-14T07:00:56.524785Z","shell.execute_reply":"2023-07-14T07:01:03.511794Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"14732"},"metadata":{}}]},{"cell_type":"code","source":"summary_token_len = len([tokenizer.encode(s) for s in dataset_samsum['train']['summary']])\nsummary_token_len","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:03.514853Z","iopub.execute_input":"2023-07-14T07:01:03.515246Z","iopub.status.idle":"2023-07-14T07:01:05.616601Z","shell.execute_reply.started":"2023-07-14T07:01:03.515211Z","shell.execute_reply":"2023-07-14T07:01:05.615557Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"14732"},"metadata":{}}]},{"cell_type":"markdown","source":"# Convert example into feature ","metadata":{}},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n    \n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n        \n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:05.617979Z","iopub.execute_input":"2023-07-14T07:01:05.620019Z","iopub.status.idle":"2023-07-14T07:01:05.627835Z","shell.execute_reply.started":"2023-07-14T07:01:05.619980Z","shell.execute_reply":"2023-07-14T07:01:05.625991Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:05.629804Z","iopub.execute_input":"2023-07-14T07:01:05.630684Z","iopub.status.idle":"2023-07-14T07:01:14.154435Z","shell.execute_reply.started":"2023-07-14T07:01:05.630649Z","shell.execute_reply":"2023-07-14T07:01:14.153447Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27aa3bf669442d09ed9851df28f0412"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45d7653f06f241e8b011767faad71869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae257294aa54c01a58ff257472d33cb"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum_pt['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:35:10.515309Z","iopub.execute_input":"2023-07-14T07:35:10.516938Z","iopub.status.idle":"2023-07-14T07:35:10.527804Z","shell.execute_reply.started":"2023-07-14T07:35:10.516897Z","shell.execute_reply":"2023-07-14T07:35:10.526786Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'id': '13818513',\n 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.',\n 'input_ids': [12195,\n  151,\n  125,\n  7091,\n  3659,\n  107,\n  842,\n  119,\n  245,\n  181,\n  152,\n  10508,\n  151,\n  7435,\n  147,\n  12195,\n  151,\n  125,\n  131,\n  267,\n  650,\n  119,\n  3469,\n  29344,\n  1],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1],\n 'labels': [12195, 7091, 3659, 111, 138, 650, 10508, 181, 3469, 107, 1]}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Collator ","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:14.156480Z","iopub.execute_input":"2023-07-14T07:01:14.157448Z","iopub.status.idle":"2023-07-14T07:01:14.164631Z","shell.execute_reply.started":"2023-07-14T07:01:14.157410Z","shell.execute_reply":"2023-07-14T07:01:14.163624Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:14.166604Z","iopub.execute_input":"2023-07-14T07:01:14.167538Z","iopub.status.idle":"2023-07-14T07:01:14.216663Z","shell.execute_reply.started":"2023-07-14T07:01:14.167503Z","shell.execute_reply":"2023-07-14T07:01:14.215517Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Training ","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:14.219345Z","iopub.execute_input":"2023-07-14T07:01:14.220623Z","iopub.status.idle":"2023-07-14T07:01:14.243198Z","shell.execute_reply.started":"2023-07-14T07:01:14.220578Z","shell.execute_reply":"2023-07-14T07:01:14.242225Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"trainer_args = TrainingArguments(\n    output_dir='pegasus-samsum', \n    num_train_epochs=1, \n    warmup_steps=500,\n    per_device_train_batch_size=1, \n    per_device_eval_batch_size=1,\n    weight_decay=0.01, \n    logging_steps=10,\n    evaluation_strategy='steps', \n    eval_steps=500, \n    save_steps=1e6,\n    gradient_accumulation_steps=16\n) ","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:14.245143Z","iopub.execute_input":"2023-07-14T07:01:14.246159Z","iopub.status.idle":"2023-07-14T07:01:14.255468Z","shell.execute_reply.started":"2023-07-14T07:01:14.246118Z","shell.execute_reply":"2023-07-14T07:01:14.254255Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model=model_pegasus, args=trainer_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:14.257754Z","iopub.execute_input":"2023-07-14T07:01:14.258949Z","iopub.status.idle":"2023-07-14T07:01:14.308816Z","shell.execute_reply.started":"2023-07-14T07:01:14.258914Z","shell.execute_reply":"2023-07-14T07:01:14.307745Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:01:14.310893Z","iopub.execute_input":"2023-07-14T07:01:14.311818Z","iopub.status.idle":"2023-07-14T07:35:10.509069Z","shell.execute_reply.started":"2023-07-14T07:01:14.311782Z","shell.execute_reply":"2023-07-14T07:35:10.507847Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshykatislam139\u001b[0m (\u001b[33mbrac\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230714_070117-6jorramc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/brac/huggingface/runs/6jorramc' target=\"_blank\">ethereal-fire-3</a></strong> to <a href='https://wandb.ai/brac/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/brac/huggingface' target=\"_blank\">https://wandb.ai/brac/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/brac/huggingface/runs/6jorramc' target=\"_blank\">https://wandb.ai/brac/huggingface/runs/6jorramc</a>"},"metadata":{}},{"name":"stderr","text":"You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [920/920 33:18, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.699700</td>\n      <td>1.483445</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=920, training_loss=1.8214837810267572, metrics={'train_runtime': 2035.3663, 'train_samples_per_second': 7.238, 'train_steps_per_second': 0.452, 'total_flos': 5526961323663360.0, 'train_loss': 1.8214837810267572, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"score = calculate_metric_on_test_ds(\n    dataset_samsum['test'], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n)\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n\npd.DataFrame(rouge_dict, index = [f'pegasus'] )","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:45:20.065594Z","iopub.execute_input":"2023-07-14T07:45:20.066609Z","iopub.status.idle":"2023-07-14T07:54:28.409867Z","shell.execute_reply.started":"2023-07-14T07:45:20.066569Z","shell.execute_reply":"2023-07-14T07:54:28.399818Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"100%|██████████| 410/410 [09:05<00:00,  1.33s/it]\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"           rouge1    rouge2    rougeL  rougeLsum\npegasus  0.018525  0.000296  0.018426   0.018393","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.018525</td>\n      <td>0.000296</td>\n      <td>0.018426</td>\n      <td>0.018393</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"model_pegasus.save_pretrained(\"pegasus-samsum-model\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:55:51.714986Z","iopub.execute_input":"2023-07-14T07:55:51.715508Z","iopub.status.idle":"2023-07-14T07:55:58.195066Z","shell.execute_reply.started":"2023-07-14T07:55:51.715469Z","shell.execute_reply":"2023-07-14T07:55:58.193857Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Save tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer.save_pretrained(\"tokenizer\")     ","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:55:58.207927Z","iopub.execute_input":"2023-07-14T07:55:58.210156Z","iopub.status.idle":"2023-07-14T07:55:58.648631Z","shell.execute_reply.started":"2023-07-14T07:55:58.210109Z","shell.execute_reply":"2023-07-14T07:55:58.646466Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/spiece.model',\n 'tokenizer/added_tokens.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test ","metadata":{}},{"cell_type":"code","source":"dataset_samsum = load_dataset(\"samsum\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:55:58.653037Z","iopub.execute_input":"2023-07-14T07:55:58.654352Z","iopub.status.idle":"2023-07-14T07:55:59.633717Z","shell.execute_reply.started":"2023-07-14T07:55:58.654305Z","shell.execute_reply":"2023-07-14T07:55:59.632778Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca5b14425114ce7807c7efe7f227850"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:55:59.635864Z","iopub.execute_input":"2023-07-14T07:55:59.636549Z","iopub.status.idle":"2023-07-14T07:55:59.875772Z","shell.execute_reply.started":"2023-07-14T07:55:59.636510Z","shell.execute_reply":"2023-07-14T07:55:59.874693Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\nreference = dataset_samsum[\"test\"][0][\"summary\"]","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:56:08.264162Z","iopub.execute_input":"2023-07-14T07:56:08.265008Z","iopub.status.idle":"2023-07-14T07:56:08.272663Z","shell.execute_reply.started":"2023-07-14T07:56:08.264970Z","shell.execute_reply":"2023-07-14T07:56:08.271388Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n\npipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:56:15.534672Z","iopub.execute_input":"2023-07-14T07:56:15.535090Z","iopub.status.idle":"2023-07-14T07:56:43.426780Z","shell.execute_reply.started":"2023-07-14T07:56:15.535060Z","shell.execute_reply":"2023-07-14T07:56:43.425540Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"\nprint(\"Dialogue:\")\nprint(sample_text)\n\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T07:56:43.431349Z","iopub.execute_input":"2023-07-14T07:56:43.432242Z","iopub.status.idle":"2023-07-14T07:57:00.702854Z","shell.execute_reply.started":"2023-07-14T07:56:43.432200Z","shell.execute_reply":"2023-07-14T07:57:00.701792Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda can't find Betty's number. Larry called her last time they were at the park together. Hannah would rather she text him. Amanda will text him.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}