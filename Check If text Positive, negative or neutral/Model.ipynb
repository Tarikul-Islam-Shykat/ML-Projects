{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scikit Learn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U scikit-learn"
      ],
      "metadata": {
        "id": "NiNWQTRwRny5",
        "outputId": "27f1c913-c7f6-4310-8594-f954c6fa130a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "-nXY92VpKl-t"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/Books_small_10000.json\""
      ],
      "metadata": {
        "id": "aPlWpdvyQLlO"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class Sentiment:\n",
        "  negative = \"NEGATIVE\"\n",
        "  neutral = \"NEUTRAL\"\n",
        "  positive = \"POSITIVE\"\n",
        "\n",
        "class Review:\n",
        "  def __init__(self, text, star):\n",
        "      self.text = text  # the written review\n",
        "      self.star = star # the given star\n",
        "      self.sentiment = self.get_sentiment()\n",
        "  \n",
        "  def get_sentiment(self):\n",
        "    if self.star <= 2:\n",
        "      return Sentiment.negative\n",
        "    elif self.star == 3 :\n",
        "      return Sentiment.neutral\n",
        "    else: \n",
        "      return Sentiment.positive\n",
        "\n",
        "class ReviewContainer:\n",
        "    def __init__(self, reviews):\n",
        "        self.reviews = reviews\n",
        "        \n",
        "    def get_text(self):\n",
        "        return [x.text for x in self.reviews]\n",
        "    \n",
        "    def get_sentiment(self):\n",
        "        return [x.sentiment for x in self.reviews]\n",
        "        \n",
        "    def evenly_distribute(self):\n",
        "        negative = list(filter(lambda x: x.sentiment == Sentiment.negative, self.reviews))\n",
        "        positive = list(filter(lambda x: x.sentiment == Sentiment.positive, self.reviews))\n",
        "        positive_shrunk = positive[:len(negative)]\n",
        "        self.reviews = negative + positive_shrunk\n",
        "        random.shuffle(self.reviews)"
      ],
      "metadata": {
        "id": "msZ6iLz0Sdvu"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = []\n",
        "\n",
        "with open (file_name) as f: \n",
        "  for line in f:\n",
        "    #print(line)\n",
        "    review = json.loads(line) # as its a json data , so you have to load it with json\n",
        "\n",
        "    #print(review['reviewText']) # and we want the reviewText so you can find that in this manner. \n",
        "    #print(review['overall'])\n",
        "\n",
        "    reviews.append(Review(review['reviewText'], review['overall']))  # this line will add all the reviews and star in your file"
      ],
      "metadata": {
        "id": "j2iaxSSYQOgW"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[6].star # you get the particular star"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P661l0FRQUD",
        "outputId": "74e32bb6-1677-4d84-fac6-7443dc74019b"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[6].text #you get the particular review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "4c-dl4UYTDFV",
        "outputId": "b77dbd27-7f07-4535-d95b-27c3b054e9c1"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The book has the fevered intensity of Oliver Stone\\'s movie \"JFK\", as the author tracks his own descent into the tangle of conspiracy theories and suspects that surround the DB Cooper hijacking case.  The book itself bounces back and forth in time, alternating between the hijacking itself, the author\\'s journey, and the lives of the various suspects.  It becomes difficult to keep track of the characters in the story, and the book ultimately spirals away into paranoia.  An unsatisfying read that ends in a shoulder shrug that obscures more than it reveals.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(reviews) # total number of reviews. "
      ],
      "metadata": {
        "id": "HmWkDQ--UoX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6b6e4b-acab-442d-8e5b-ab60cfe78004"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prep data"
      ],
      "metadata": {
        "id": "BysBqEHEQZil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split "
      ],
      "metadata": {
        "id": "QPGN20c6P-_x"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "somnetimes  we train the model with entire data set and thats not a good staytegy , so you split the data set in two part where part of the data set is used for training and part of them  is used to test your model. \n",
        "\n",
        "SO HERE HOW IT WORKS\n",
        "\n",
        "you need to train your machuine learning model. so you train you model with 80% of your data and the other 20% is for testing the model"
      ],
      "metadata": {
        "id": "knPN4GOkUGqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training, test = train_test_split(reviews, test_size= 0.33, random_state = 42)\n",
        "\n",
        "train_container = ReviewContainer(training)\n",
        "\n",
        "test_container = ReviewContainer(test)"
      ],
      "metadata": {
        "id": "d2Eg-k6mQzSW"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_container.evenly_distribute()\n",
        "train_x = train_container.get_text()\n",
        "train_y = train_container.get_sentiment()\n",
        "\n",
        "test_container.evenly_distribute()\n",
        "test_x = test_container.get_text()\n",
        "test_y = test_container.get_sentiment()\n",
        "\n",
        "print(train_y.count(Sentiment.positive))\n",
        "print(train_y.count(Sentiment.negative))\n"
      ],
      "metadata": {
        "id": "0jqMZlCaarco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a780031f-5cc9-42de-c0ff-fd8e6786e636"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "436\n",
            "436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of word vectorization**"
      ],
      "metadata": {
        "id": "cvEu7_3f331_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ],
      "metadata": {
        "id": "wcuLOpEOSaaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of sklearn bags of vectorization, this cell is not a part of the code\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "     'This is the first document.',\n",
        "     'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "     'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(\"unique words : \")\n",
        "print(vectorizer.get_feature_names_out(),'\\n')\n",
        "\n",
        "print(\"tokenization of words in matrix : \")\n",
        "print(X.toarray(),'\\n')\n",
        "\n",
        "# to select more than 1 word\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(\"unique words of two : \")\n",
        "print(vectorizer2.get_feature_names_out(),'\\n')\n",
        "\n",
        "print(\"tokenization of words in matrix : \")\n",
        "print(X2.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwdTOU8ukBLK",
        "outputId": "27e459fd-5c61-4174-ca81-1350899dca17"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique words : \n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this'] \n",
            "\n",
            "tokenization of words in matrix : \n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]] \n",
            "\n",
            "unique words of two : \n",
            "['and this' 'document is' 'first document' 'is the' 'is this'\n",
            " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
            " 'this document' 'this is' 'this the'] \n",
            "\n",
            "tokenization of words in matrix : \n",
            "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#starts from here\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "zCS-vdp633mf"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming the the word in bag of vectorization\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit_transform(train_x) # it make all the text in the token in a token format 0/1\n",
        "\n",
        "test_x_vectors = vectorizer.transform(test_x)"
      ],
      "metadata": {
        "id": "Yr2sy6z1Sg2Q"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuC7F5YUTQzS",
        "outputId": "f072a1ba-9903-44e4-8933-28d96ce749bd"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A well-written chronically of what shaped Senator Warren into a person who never gives up a righteous fight.  It is never boring but truly inspiring.  I've tried to do some worthwhile things in my life, but in comparison to Elizabeth Warren I'm a slacker.  Thank God for Senator Warren and her very large band of assistants and backers.  I'm ready to read her other books.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now see it in the token form where it transform into a matrix."
      ],
      "metadata": {
        "id": "pxC5_qaTTUHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_vectors = vectorizer.fit_transform(train_x)\n",
        "print(train_x_vectors[0].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYjLUj_NTFkw",
        "outputId": "7e406efe-3913-4507-e752-4d5edfe87d16"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification**"
      ],
      "metadata": {
        "id": "ulOmCvgo10Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear SVM**"
      ],
      "metadata": {
        "id": "RZTk1_3v14xF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting using linear svm classsification"
      ],
      "metadata": {
        "id": "ife22-bV-wx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "PoU27Xns138u"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_svm = svm.SVC(kernel = 'linear')\n",
        "clf_svm.fit(train_x_vectors, train_y)\n",
        "\n",
        "print(test_x[0])\n",
        "print(clf_svm.predict(test_x_vectors[0]))\n",
        "print()\n",
        "\n",
        "print(test_x[7])\n",
        "print(clf_svm.predict(test_x_vectors[7]))\n",
        "print()\n",
        "\n",
        "print(test_x[8])\n",
        "print(clf_svm.predict(test_x_vectors[8]))\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmGuohB21z7A",
        "outputId": "50dcb45c-d0d4-46ef-ac2c-621e520eeedb"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First of all, this book needs some serious editing.  This book probably should not have been sold in it's  current version.  The story itself is a little unbelievable.  Billionaire brothers who beat up their fathers don't exist in most people's lives.  The storyline or what I could make out to be the storyline is really weak.  Save your money.\n",
            "['NEGATIVE']\n",
            "\n",
            "Not really impressed. Finished reading it because I purchased it. The misspelled words and weird phrases were very distracting to me.\n",
            "['NEGATIVE']\n",
            "\n",
            "I really have enjoyed this series and can't wait for the next one to come out. I especially liked that the author gave Kid and Jason their own stories, even though they occurred at the same time. She could have easily put them both into one book and jumped between locations. I am so glad she didn't.The last two installments have not had as much sensuality as the previous 3 books, but the story line is there, so you don't necessarily miss it. I do like how she has not just dropped the other characters in sacrifice to the current beau.This is a great series.\n",
            "['POSITIVE']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree**"
      ],
      "metadata": {
        "id": "Of4xK4RS4P1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting using decision tree classifier"
      ],
      "metadata": {
        "id": "foWdsO-n-pzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "F2PfzzkC4qU9"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_dec = DecisionTreeClassifier()\n",
        "clf_dec.fit(train_x_vectors, train_y)\n",
        "\n",
        "\n",
        "for i in range (5,10):\n",
        "  print(clf_dec.predict(test_x_vectors[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbRD1Wrs-PGJ",
        "outputId": "ad348690-da67-4c81-a892-4e20fc2797cf"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['POSITIVE']\n",
            "['NEGATIVE']\n",
            "['NEGATIVE']\n",
            "['NEGATIVE']\n",
            "['POSITIVE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "EhuHEoLG4fn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using naive bayes classificaton"
      ],
      "metadata": {
        "id": "BQY_ak2C-1sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "L4SUQDa5-2G1"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clf_gnb = GaussianNB()\n",
        "#clf_gnb.fit(train_x_vectors, train_y)\n",
        "#for i in range (5,10):\n",
        "  #print(clf_gnb.predict(test_x_vectors[i]))"
      ],
      "metadata": {
        "id": "3I0XA0KQ-8iN"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "NL9k76Td4jSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using logistic regression"
      ],
      "metadata": {
        "id": "UuHUkp7k_5zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "PziO654h_43t"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_log = LogisticRegression()\n",
        "clf_log.fit(train_x_vectors, train_y)\n"
      ],
      "metadata": {
        "id": "x1Y81FpOAP6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74f1ef7-a25a-4afe-bdd9-5ced9f5c88fe"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (5,10):\n",
        "  print(clf_log.predict(test_x_vectors[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsqifM-CAouc",
        "outputId": "4f5627e8-db52-48d6-b94b-d668b450d11d"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NEGATIVE']\n",
            "['NEGATIVE']\n",
            "['NEGATIVE']\n",
            "['POSITIVE']\n",
            "['NEGATIVE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "06PvwGeWBOl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See which model is  doing better / providing more accuracy"
      ],
      "metadata": {
        "id": "EftgN4ldBUJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(clf_svm.score(test_x_vectors, test_y))\n",
        "print(clf_dec.score(test_x_vectors, test_y))\n",
        "#print(clf_gnb.score(test_x_vectors, test_y))\n",
        "print(clf_log.score(test_x_vectors, test_y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVNc6AwkBaf7",
        "outputId": "d4849391-61b0-40d0-be25-0eccb321312e"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7980769230769231\n",
            "0.6418269230769231\n",
            "0.8149038461538461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1 SCORE**\n",
        "\n",
        "F1-score is one of the most important evaluation metrics in machine learning. It elegantly sums up the predictive performance of a model by combining two otherwise competing metrics — precision and recall\n"
      ],
      "metadata": {
        "id": "ULFq9_R0Kx-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "jFJBfBGQKxKT"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score for svm "
      ],
      "metadata": {
        "id": "1pJuVSkwOB96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels = [Sentiment.positive,Sentiment.neutral, Sentiment.negative])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBhFtqnGLASk",
        "outputId": "0c5a9e1a-8f7f-4cfb-d995-43fc4f74ba19"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8028169 , 0.        , 0.79310345])"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score for decision tree"
      ],
      "metadata": {
        "id": "9VMSGgkbOMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(test_y, clf_dec.predict(test_x_vectors), average=None, labels = [Sentiment.positive,Sentiment.neutral, Sentiment.negative])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dI24efnOD5x",
        "outputId": "085c30e4-fb71-497d-eb16-799f63803858"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.64439141, 0.        , 0.63922518])"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score for logistic regression"
      ],
      "metadata": {
        "id": "h_FtdfM2OOnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(test_y, clf_log.predict(test_x_vectors), average=None, labels = [Sentiment.positive,Sentiment.neutral, Sentiment.negative])\n"
      ],
      "metadata": {
        "id": "bzhKodT1OHZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0ce72c-dd83-47de-bd52-66962bdb762b"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.82051282, 0.        , 0.808933  ])"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y[0 : 10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9tBOAzbOJkK",
        "outputId": "8b6cddf0-3062-445c-f442-360b67b2633c"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['POSITIVE',\n",
              " 'NEGATIVE',\n",
              " 'POSITIVE',\n",
              " 'POSITIVE',\n",
              " 'POSITIVE',\n",
              " 'POSITIVE',\n",
              " 'NEGATIVE',\n",
              " 'POSITIVE',\n",
              " 'POSITIVE',\n",
              " 'POSITIVE']"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = ['bad very good', \" great\", 'horrible waste of time']\n",
        "new_test = vectorizer.transform(test_set)\n",
        "\n",
        "clf_svm.predict(new_test)"
      ],
      "metadata": {
        "id": "MwBk7vHNOiwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65fa26aa-0cda-4e2e-88e2-7377e5095314"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['NEGATIVE', 'POSITIVE', 'NEGATIVE'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    }
  ]
}